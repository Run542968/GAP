#### 第一次大版本，20230923最后一次提交到github，comit id=`firtst backup`
- [x] 从Tad_eval.py保存下来的结果，看到start-time存在复数，需要debug
  - [x] 问题应该是处在后处理，后处理有一步骤是把(center,width)格式的结果转换为(start,end)的步骤：`segment_cw_to_t1t2`，如果width>center, 那么就会出现负值
  - [x] 解决方案是优化对`segment_cw_to_t1t2`的结果进行clamp()，如果出现负值或者超过最大值就截断
  - [x] 考虑到`segment_cw_to_t1t2`总是对经过sigmoid()后的坐标值进行处理的，所以直接在每个用到`segment_cw_to_t1t2`的地方加`clamp(min=0,max=1)`
- [x] CLIP模型的输出的half精度的(float16)，需要手动改成float(float32)
- 发现condional DETR在Thumos14上，用CLIP特征，在close-set的情况下，不管用one-hot (target_type='none')还是prompt，性能都特别差。此时对视频特征的处理是拿整个video放进来训练，不做插值或者resize
  - 推测可能以下原因：
    1. 训练的batch_size太小（因为Thumos14视频太长了，batch_size设置为2），导致训练loss下不去
        - [x] 换显存大的卡试试，同样的设置下，加大batch_size确实能涨点，目前只测试了从batch_size: 2->4 (Thumos14_CLIP_none_15->Thumos14_CLIP_none_17)，测试了batch_size=16的实验，发现影响也不是很大
        - [ ] 添加梯度累积
    2. detr在object detection的时候，输入图片是resize到统一的大小的，这样就使得query在**不同数据学到的位置尺寸是统一的**，输入大小是固定的，而不是一会大一会小
        - [x] 最简单的方式，把特征线性插值到统一的长度，没什么太大影响，这种方式还是没解决小目标的问题
        - [x] 对Thumos14的特征分段处理，参考TadTR
          - 🚩因为Thumos14视频内部的实例数量太不均匀了，有的特别多，有的又特别少，这个问题可能才是根本原因。对于实例多的，负样本不足，对于实例少的，正样本不足
          - 😀**非常有效**
    3. 通过观察在Thumos14上的结果来看，分类是挺棒的，问题主要出现在定位上。考虑到thumos14有很多小目标，这应该是制约性能的主要因素
        - [x] 测试一下re-scale到同一尺度
          - 能有一定的作用，但不明显（Thumos14_CLIP_none_14->28,29）
        - [x] 发现loss只有一开始下降，然后就不下降了，Giou loss一直比较高。试一下增加giou和bbox loss的权重，把学习重心放在回归上 (Thumos14_CLIP_none_17->Thumos14_CLIP_none_25)
          - 没什么用，loss反而还下降了，性能也下降了
          - 这个实验做错了，按照4的分析，应该是L1 loss没训练好，也就是中心没找对（这里分析错了，应该是IOU没学好，现在的结果大部分都是小框）
          - [x] 增大L1 loss的权重
            - 在Thumos14上影响不大Thumos14_CLIP_none_37，38，39
        - [x] 增加一个local的Conv1D：确实是有效的(Thumos14_CLIP_none_17->18)，目前只增加了一层Conv1D (Thumos14_CLIP_none_14->15, Thumos14_CLIP_none_17->24)
    4. 小目标的问题
        - [x] 如果ActivityNet13上面的性能比较不错，那么说明Thumos14的小目标太多了，很难收敛。怪不得TadTR要重新crop
          - 分析一下就是，一个动作实例只有1s(25s-26s)，而完整的视频duration是500s，这样归一化到[0,1]得到的width只有0.002，诶，但是center是正常的，现在的结果width预测的还行，center不正常
          - 应该是IOU loss没学好
    5. 探究一下模型结构
       - [x] 把encoder和decoder放小点，轻量一点
         - 在Thumos14上影响不大（Thumos14_CLIP_none_41，42，43），在ActivityNet13也只能说影响没那么大，是有影响的
- Thumos14结果的观察
  - 观察发现回归的结果，宽度预测的挺好的，但是中心点预测的不好（预测的时序区间的长度差不多是对的，但是开始时刻是有问题的，转换到(center,width)的格式就是center预测的不好）
  - 分析一波，感觉是因为后处理得到proposal的时候只考虑到了分类置信度，所以得到的proposal没有考虑到位置的score
- ActivityNet13结果的观察
  - [ ] 在zero-shot的时候，分类仍然是个大问题，很多类别是直接分错的，**训练阶段应该很有必要融入文本，类似UnLoc**。仅仅作为target的话，肯定会让classification head在seen classes上过拟合
- [x] 目前在visual feat和text feat计算相似度的时候还有一个scale_logits的参数没用，还需要计算normalization, 可以试一下
  - [x] 😀有用的一批，正向作用：ActivityNet13_CLIP_prompt_zs_10->ActivityNet13_CLIP_prompt_zs_13；反向作用：ActivityNet13_CLIP_prompt_zs_6->ActivityNet13_CLIP_prompt_zs_12
- [x] 测试一下随机种子能不能固定：可以固定
- [x] 🚩探究一下不同显卡之间会不会有性能差异: ActivityNet13_CLIP_prompt_zs_6(4090), ActivityNet13_CLIP_prompt_zs_7(3090), ActivityNet13_CLIP_prompt_zs_8(1080 Ti)
  - 😵确实有影响，在1080 Ti的性能最好？
- 🎈添加一层transfer wrapper，用来直接的进行语义转换，也就是计算一个转移矩阵$A\in R^{u\times s}$，满足$(u,s)\times(s,dim)= (u,dim)$
  - [x] 最简单的方式，$(u,s)=(u,dim)\times(dim,s)$
    - 失败了，性能几乎没有。分析一下原因，因为这种简单方式计算得到的(u,s)可以说是标准答案，而训练过程中的semantic head是很难完美的把预测学成seen class的样子的
  - [x] 重新分析了一下，现在改成了wrapper_v2，也就是先转换成seen的预测结果(完整的经过sigmoid的预测结果)，然后再映射到unseen
    - 不出预料也很差
- [ ] 研究一下clip_pkg.tokenize(act_prompt).long().to(device)的输出到底是什么
- [x] 🚩注意现在用的description_v4，在conditional_detr.py获得文本描述的地方需要加一个索引，目前默认是0
- [x] 后处理的时候先计算预测概率最大的类别，然后只获得这个类别的queries作为proposals，这是一个非常强的先验，--postprocess_type 'class_one' --postprocess_topk 1, `ActivityNet13_CLIP_prompt_zs_27`
  - 有一些掉点
- [x] 跑一下ActivityNet13在close_set用norm_scale->ActivityNet13_CLIP_prompt_2
  - [x] 反而训练崩了，不如去掉norm_scale，应该是参数的问题，调个学习率试一下
- [x] ActivitiNet13跑多几个种子->ActivityNet13_CLIP_prompt_zs_44,50,51
  - 差距不大，上下1个点的波动
- [ ] 时序建模非常重要，EfficientPrompt那篇文章也说了，类别之间的差异会影响时序的建模。而且是在CLIP的特征上，CLIP的特征是用来做文本alignment的，Actionness很差。而我们的监督信号又是在seen classes的，这会导致一种class bias，也就是建模时序的时候会融入类别语义，从而导致在unseen class表现不好
  - [ ] 训练的时候不只有时序监督，还有类别监督。类别监督的信息用到的不多，如何在训练的时候就和测试的情况一样，参考meta zero-shot detection那篇文章？
- 语义的gap问题，训练得到的semantic head是偏向于seen classes的，怎么迁移这个语义gap
  - [x] 之前的一个尝试是上面的transfer wrapper，但是失败了
- [x] 增加在训练集上inference的结果
- 目前存在的问题是时序建模会破坏CLIP原有的语义信息，导致语义特征出现问题，从而很难泛化的很好
  - [x]所以后面决定分两个分支来做
- [x] 😍不破坏原有的模型，仅仅在测试阶段引入了ROIalign，把box对应的原始CLIP特征提取出来，然后average pooling到一个embedding，再和文本计算相似度，进行分类。把ROIalign的classification score和detector的classification score通过加权聚合的方式ensemble一下作为最后的对这个proposal的分类预测得分
  - 失败了，首先两个分数的scale不一致，融合存在问题，预测结果是NaN
    - [x] 找到了预测结果是NaN的原因，因为数据中存在padding，应该用mask后的T, 而不是batch的T作为每个视频的长度 
  - [x]几何平均的融合方式需要先sigmoid，保证值不为负数，然后再融合
    - 在ActivityNet有一定提点，大概1个点ActivityNet13_CLIP_prompt_zs_64-69
- 对dataset进行了删减处理
  - [x] 删掉了get_feature那里padding 0的操作，没什么意义
  - [x] 删掉了Thumos14在parse_anno中保存feature_length的操作，这个没什么用。如果非要加的话（会设置为slice_size），会存在误解，也就是slice < slice_size的时候特征长度并不是slice_size
    - [x] 一个很棒的处理方式。在dataset调用__getitem__的时候，其实是拿到了这个视频的真实特征长度的（在padding之前），通过一个hook的方式在__getitem__的部分为valid_anno_dict添加一个['snippet_length']属性，虽然后续也没什么用的🤣
  - [x] 处理完之后跑了一个实验`Thumos14_CLIP_prompt_zs_8frame_32`
    - 没啥用
- [x] 跑一下在Thumos14上不同overlap
  - 有影响，但跑的几个参数都在掉点Thumos14_CLIP_prompt_zs_8frame_33-40
- [x] 改了backbone，以后要加上backbone进行实验，时序建模应该是非常关键的，特别是对Thumos14来讲
  - [x] 实验效果记录一下
  - 好像没什么大用，在二分类上加了都没效果，顶多保证不掉大点
- [x] 添加一个语义分割的loss,直接作用在每一个snippet上
  - [x] 需要在dataset那里生成target['segmentation_onehot_labels']作为segmentation的标签, 维度为[feat_length, num_classes]
  - [x] 注意这里每个segmentation维度[T,num_classes]的T是严格按照特征长度的，所以会出现batch内部T不一致的情况，即 T < slice_size的情况，所以**计算loss的时候要先补全才能批处理**
    - 好像没啥用啊，崩溃了😩
- [x] 完善方法的class-agnostic的二分类detector，完全解耦分类和定位
  - 只进行二分类是非常有效的
- [x]🚀**代码总结**：
  - 此时的代码主要基于DETR架构，每一个queries既要负责定位，也要负责语义分类
  - args.binary的设置是把类别总数变为1，也就是训练一个class-agnostic的DETR
  - 代码中还包含一些语义分割的结构，在memory上面构建一个snippet-level的分类

#### 第二次大版本，20230927最后一次提交到github，comit id=`second backup`
- 代码结构大改，这一版本去除了--binary的参数，直接默认DETR的query只负责前景定位，预测得到的分数是proposal的质量分数。🧨不管是“close_set”还是“zero_shot”都改！！
  - 🔒特别注意，除了`target_type="none"`，否则都默认DETR的query只定位前景
  - `target_type="none"`就是传统的DETR范式用在TAL，那当然是“close_set”了
- 分类用语义分割的结构来做，following `2023_ICCV_EdaDet`
- 还去除了一些失败的尝试：`wrapper`
- [x]完善二分类detector和语义分类两个头的代码
  - 简单来讲就是detr的detector只用来检测前景框，完全不考虑分类，输出的是proposal的前景分数（也就是**质量分数**），这个分数应该要被用到
  - 分类用另一个分支的语义来做，默认采用在CLIP的视觉特征上进行ROIalign
- [x] 引入segmentation_loss反而掉点的原因应该是在memory上加的分类约束，使得memory又包含了一些语义信息，导致后面的decoder不能很好的检测到class-agnostic的proposals。
  - [x] 彻底解耦，引入一个新的分支来做语义分类(不影响detector)，ROIalign在这个分支上做
    - [x] 最简单的做法，直接在clip_feat上面加一层MLP进行语义的映射
    - [x] 复杂一点就是加一层self-attention
      - 😀这个效果最好,Thumos14_CLIP_prompt_zs_8frame_v2_7
      - 但是这种需要训练的方式性能还是比不上直接拿CLIP的visual和text特征进行计算(Thumos14_CLIP_prompt_zs50_8frame_v2_7)，这样做训练出来的classification一定是过拟合的。特别是在训练类别数量不足的时候，zero-shot能力很差的
    - [x] 考虑到语义的local特性，加Conv1D或许会更好？
  - [ ] segmentation_loss的softamx加个温度系数$\tau=0.07$
    - 不加了，这样用segmentation loss进行base classes的训练一定会导致过拟合
- [ ] 借鉴一下ODISE的grounding loss, 完成ROIalign得到的segment和文本中word的相似度计算约束
- [x] 是否显示约束一下背景类？Following `DetPro`
  - [x] 目前的segmentation loss已经实现了这个功能：对于背景snippet，它对于所有base类的预测都是$\frac{1}{C_{base}}$。
    - 但是好像没什么用
  - 继续拓展一下，构造一个learnable background embedding
    - [x] 在V3版本构造了背景embedding，但很难发挥重要的作用
- [ ] 像actionCLIP一样进行prompt增广
  - [x] V3版本合并了prompt和一些sub-action的文本，用了直接平均的方式，效果比单独用prompt差几个点
- [x] 实验发现ROIalign strategy那里，不管是先预测再crop，还是先crop再预测，结果几乎是一样的，差别非常小。分析觉得是ROIalign的时候选了max，选到了最显著的特征
- [ ] 试试把ActivityNet的特征也处理一下？
  - [ ] 整个视频只采样278帧
  - [ ] 每个snippet用8帧来group
#### 第三次大版本，20231005最后一次提交到github，comit id=`third backup`
- V3版本-Summary
  - [x] 🚩所有的实验都带后缀v3
  - [x] 首先是抛弃segmentation loss，经过第二版本的分析，只要在base classes上训练，必然导致网络过拟合，丢失CLIP的泛化能力
- [x] 😎接下来是在segment-level进行时序建模，语义建模等操作
  - [x] 训练阶段：
    - visual: 把一个batch内的所有动作实例都用ROIalign给crop出来，然后在最原本的CLIP visual特征上加一层时序/语义建模网络，再pooling得到一个visual embedding
    - text: 对于每个类别，提取每个子动作的特征，然后concat，再过一层时序/语义建模网络, 再pooling得到一个text embedding
      - 这种方法也算一种fine-grained的类别建模，即相似动作子动作不同
    - Loss: **先来最简单的**，分类的cross entropy，从viusal→text，一对多
  - [x] 测试阶段：
    - 用预测的结果ROIalign出visual segment，然后和训练流程保持一致，得到proposal logits
  - 🔍结果分析：
    - 效果特别差，instance loss特别低，几乎为0
    - 有可能是和detector的学习率不匹配
      - [x] 增加instance loss权重
        - 没用，问题应该不在这里，Thumos14_CLIP_description_zs50_8frame_v2_5,6,7,8
      - [x] 在optimizer那里为instance loss的文本和视觉self-attention单独设置参数学习率
    - 有可能是sub-action的文本语义太过于凌乱
      - [x] 在文本这边，除了concat子动作的文本embedding，在最开始拼接一个类别名称的prompt
      - [x] 经过self-attention后，只拿第一个类别名称的prompt作为文本embedding(这样做的出发点是self-attention已经把sub-action信息聚合到了class name)。只训练文本这边，visual那边固定住。单纯的学习怎么把sub-action聚合到class-name中
        - 经过训练的方式还是没有不训练的好，即使初衷是想要学习怎么聚合，但实际上还是没见过novel类，还是不会聚合novel类
    - 看了一下结果，所有的proposals都被预测成了几个固定的类别
    - 可能是没加residual connect的原因，一个单独的self-attention其实完全打乱了原本CLIP的语义
      - [x] 给visual和text都加上residual connect
        - 👌非常有效，问题确实出在了这里。这批实验直接覆盖了以上没加residual connect的实验
- [x] 上面的实验只有简单的一层self-attention和residual connect，考虑采用一层transformer encoder完成建模
  - 效果确实好
- 上面的instance loss是采用了Cross Entropy Loss，实际上这个loss不太具备很好的泛化能力。因为softmax得到的是一个相对概率，即视觉特征和其他所有文本类别的相似度的相对值，BCE才是匹配的绝对值。因此把CE改成BCE试一下，直接用focal loss
  - [x] 没什么太大的影响，并没有解决关键的问题
- 分析一下Thumos14-split-50性能差的原因：
  - 首先是预测得到的bounding box质量没那么好，因为训练数据少。低质量的bounding box包含很多背景/其他类别的内容，导致CLIP对于这个中带噪的segment，很难预测出准确的类别。简单来讲就是bounding box内的类别不唯一
    - [ ] 这样来看，dense prediction是一个正确的方案。但是dense prediction的结果要怎么group成一个segment proposal是一个核心问题
    - [ ] 还是要在训练阶段训一个背景类别的embedding，然后在测试的时候dense prediction，这样背景片段就不会被错误的分为某个类
      - 具体实现：这是一个dense prediction的分支（从CLIP visual feat引出）
        - [ ] 加一层时序建模(local的一维卷积),也可以不加
        - [x] 构造一个learnable background embedding
        - [x] visual这边就别训练了，只训练text这边
          - 实验结果是没什么大用，并没有起到过滤背景snippet的作用🤦‍♂️
  - 另一个方面就是inter-class的可区分性。也就是diving和cliffdiving经过CLIP text encoder得到的语义很相似。需要在prompt的层面层架类间的区分性
    - [x] 这个需要采用的就是一种prompt augmentation的策略
- 一个新的方案：
  - [x] visual这边暂时不动，文本这边纯prompt作为一个query, 通过cross-attention来聚合visual的特征，然后输出0/1，这样来进行一个dense的捕捉
    - 具体一点：视觉prompt的维度Nxdim作为query，visual的特征维度BxTxdim作为Key和Value，把query给repert一下，经过cross-attention得到BxNxdim的结果，然后再和原本的prompt进行residual connection，经过一个MLP得到0/1匹配得分。这个loss称作matching loss
      - 没啥用，只要一学习，原本的语义分布就被改变了，就会在base类上过拟合
- [ ] 😎如果上面的work了，进阶的训练版本：
    - [ ] 可以采用对比学习
      - visual→text: intra-video负样本可以是错误的sub-action组合; inter-video是batch内其他的类别文本
      - text→visual: intra-video负样本可以是错误的ROI region; inter-video是batch内其他视频的ROI region
        - 这个的优势在于能够从CLIP的角度refine proposal，也就是给detector产生的proposal重排序，把质量低的proposal分数降低，从而提高定位质量
- [x] 另一个思路，把class_name prompt作为query，然后其他的sub-action/description作为key和value，进行cross-attention，训练的时候学习某种聚合能力
  - 进行了尝试，这样做一定程度上避免了过拟合，但还是没有增强原本的CLIP的分类能力
- [ ] 核心转为怎么提高定位的性能：
  - [x] 增加一个0/1的mask loss作为辅助loss（在memory上面加，增加encoder的能力），提升detector的性能
    - 没什么用，反而影响了原本的regress head
- [x] 不用prompt模板，只用class name得到文本embedding
  - Thumos14_CLIP_name_zs50_8frame_v2_1
  - 差距并不大，只掉了一个点。这说明class_name是具有明显的辨别信息的
- [ ] 添加评估proposal质量的指标，参考`2022_ECCV_EfficientPrompt`
  - 不用了，直接binary就可以评估proposal的质量了
  - 这个指标在消融的时候加
- [x] 前两个版本的logits_scale少了一个exp()操作，CLIP原本的模型是有exp()的，尝试一下
  - ✌对Thumos14非常有效，在仅仅使用CLIP的能力做分类的时候，能够提高性能。这是合理的，因为原本CLIP训练的时候就有这个exp操作，Thumos14_CLIP_prompt_zs50_8frame_v2_16->17
  - 🤣对activityNet13没啥用，ActivityNet13_CLIP_prompt_zs50_v2_7，ActivityNet13_CLIP_prompt_zs_v2_7。说明类**间差异比较大的情况下**，这个exp放缩操作影响不大
- [ ] Thumos14对数据处理的方式导致在测试的时候会有很多empty的video，这些video被错误的定位和分类，从而导致和binary的case相比性能掉点那么多
  - 而且最可恶的是这些empty video经过CLIP分类以后，确实是能够被分类为某个类别的
- 📚可以着手考虑的问题：
  - [ ] CLIP对于文本的global概念，没有细粒度的word理解，**不容易区分相似的动作**，例如"Diving"和“CliffDiving”
  - [ ] class-agnostic的proposal肯定是包含有背景信息，怎么去除这个背景的噪声干扰？实现更准确的分类？
  - [ ] 动作proposal是有时序性的，怎样的一种pooling(目前是ROIalign以后用简单的average pooling), 可以在保证时序性质的情况下，对齐visual和sentence，也就是静态的sentence和动态的visual怎么alignment
    - 这种简单的pooling虽然能识别一些动作，但是缺乏动态建模
  - [ ] Not action是否可以借鉴？
  - [ ] 应该最大程度的利用CLIP预训练好的alignment能力，而不是改变它
    - 一个可能的方案
      - [ ] ROIalign后的segment feat上加一层注意力层或者一维卷积，这层网络的作用是**组合视觉概念并且捕捉时序特征**，然后输出一个embedding。
      - [ ] 文本方面，利用到多个sub-action的信息，也是在sub-action上加一层网络，完成sub-action之间的**顺序建模，和概念组合**，最后输出一个embedding。
      - [ ] 用这两个embedding完成最终的ROIsegment分类，注意这里只有这一层composition network是训练的，其他所有部分都是fix的（保证了不过多改变CLIP特征的分布）
      - [ ] 还可以构造一些顺序错乱的文本作为负样本，进行对比学习
- [ ] 😶‍🌫️持续调参
  - binary的性能还能调，两个库都还能继续调
    - num_queries等参数
  - ROIalign内部还有一个output_size可以调，加了一个超参数：ROIalign_size
    - [x] 调了，但是没啥大用。没用也合理，因为不管怎么调这个参数，只是ROIalign以后得到的特征信息是否丰富而已，box的坐标就那么大。无非是多插值还是少插值

#### 第四次大版本
- 首先是整合上面几个版本的代码，并且简化。
  - [x] 相比于detector新增的loss，例如instance_loss, segmentation_loss, mask_loss, matching_loss所用的module单独写，不要交叉
    - ant13不需要norm_embed
  - [x] 把一些已经固定的参数，例如norm_embed, exp_ligits_scale放到config文件